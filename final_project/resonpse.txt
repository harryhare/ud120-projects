

1.
goal:
use machine learning and eron data to predict poi

how machine learning is useful: 
比如poi的salary可能比一般人高
poi和poi直接的通信可能比一般人间的通信频繁
可以用machine learning来印证这种假设

use 15 features totally
only 5 of them have a positive feature_importances_

outlier:
TOTAL(not real person)
FASTOW ANDREW S(lack email data)
YEAGER F SCOTT(lack email data)
HIRKO JOSEPH(lack email data)
KOPPER MICHAEL J(lack email data)

left 141 person
poi 14
no-poi 127

all 15 features are available with the 141 people


2.
features:
features I chose in this project:
 'total_payments', 'total_stock_value', 'expenses', 'other', 'salary', 'restricted_stock', 'bonus', 'to_messages', 'from_messages', 'from_poi_to_this_person', 'from_this_person_to_poi', 'shared_receipt_with_poi', 'fraction_from_poi_to_this_person', 'fraction_from_this_person_to_poi'
new features I added:
'fraction_from_poi_to_this_person', 'fraction_from_this_person_to_poi'
why I add these features:
I think fraction may do better then count.
If someone send email very often, then he may send more email to poi then others, even if he is not in close relationship with pois.

selection process：
feature_list1=["salary","restricted_stock"]
'''
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=6, min_weight_fraction_leaf=0.0,
            random_state=None, splitter='best')
	Accuracy: 0.79680	Precision: 0.20986	Recall: 0.18950	F1: 0.19916	F2: 0.19325
[ 0.12124007,  0.07353306,  0.08288174,  0.56640341,  0., 0.15594171])
'''
feature_list2=feature_list1+["bonus"]
'''
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=6, min_weight_fraction_leaf=0.0,
            random_state=None, splitter='best')
	Accuracy: 0.82793	Precision: 0.33081	Recall: 0.28400	F1: 0.30562	F2: 0.29227
[ 0.12626493,  0.        ,  0.08524709,  0.12817332,  0.45021622,   0.07863298,  0.13146547]
AdaBoostClassifier(algorithm='SAMME.R', base_estimator=None,
          learning_rate=1.0, n_estimators=50, random_state=None)
	Accuracy: 0.85113	Precision: 0.41503	Recall: 0.28450	F1: 0.33759	F2: 0.30360
	Total predictions: 15000	True positives:  569	False positives:  802	False negatives: 1431	True negatives: 12198
'''
feature_list3=feature_list2+["to_messages","from_messages",
 "from_poi_to_this_person","from_this_person_to_poi","shared_receipt_with_poi",
  new_feature1,new_feature2]
'''
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=6, min_weight_fraction_leaf=0.0,
            random_state=None, splitter='best')
	Accuracy: 0.93057	Precision: 0.51163	Recall: 0.61600	F1: 0.55898	F2: 0.59185
[('fraction_from_poi_to_this_person', 0.45804342800706016), 
('total_stock_value', 0.23487292711823565),
 ('expenses', 0.12970722666215376),
 ('from_this_person_to_poi', 0.12954555710659485),
 ('total_payments', 0.047830861105955622),
 ('shared_receipt_with_poi', 0.0), 
 ('from_poi_to_this_person', 0.0),
 ('from_messages', 0.0), 
 ('to_messages', 0.0),
 ('bonus', 0.0),
 ('restricted_stock', 0.0),
 ('salary', 0.0),
 ('other', 0.0),
 ('poi', 0.0)]
'''

scaling，why：
I use a decision tree algorithm, so I don't need to scale my data.
But in order to try other algorithm, I used MinMaxScaler to scale my data.

features importance:
([ 0.        ,  0.        ,  0.08783769,  0.22173357,  0.        ,
        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
        0.        ,  0.16128385,  0.04783086,  0.48131404])
importance of new added feature:
0.04783086,  0.48131404

3.
algorithm:
DecisionTreeClassifier

other algorithm tried：
GaussianNB(low recall)
SVC(low recall)
KNeighborsClassifier(low recall)
RandomForestClassifier(low recall)
AdaBoostClassifier(also good,but it cost a lot more time)


4.
tune parameters:
改变参数，使预测的准确率最高，
如果不调参数，几使用默认参数，可能得不到想要的结果，或者花费更多的时间计算，却得不到更好的结果
how:
gridCV

5.
validation:
mistake:
经典的错误：
在split train 和test 数据时，train和test中不同classification的比例不同。
比如poi都在train中，而test中都是nonpoi

how：
I use crossvalidation

6.

evaluation metrics:
without kfold:
('accuracy', 0.8571428571428571)
('recall', 0.80000000000000004)
('precision', 0.5)
confusion matrix:
[[26  4]
 [ 1  4]]
 
user kfold:
	Accuracy: 0.93079	
	Precision: 0.51291
	Recall: 0.61600
	F1: 0.55975
	F2: 0.59219

performance:
Accuracy:the resault of my program is 93% right.
Precision:if my program mark someone as a poi, then 50% he is a real poi.
Recall:62% of pois with be discriminated by my program.